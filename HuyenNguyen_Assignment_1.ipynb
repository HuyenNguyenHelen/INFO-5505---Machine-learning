{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HuyenNguyen_Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/HuyenNguyenHelen/INFO-5505---Machine-learning/blob/main/HuyenNguyen_Assignment_1.ipynb",
      "authorship_tag": "ABX9TyMRZp7RLzQH/a82PrHT0Htf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/INFO-5505---Machine-learning/blob/main/HuyenNguyen_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1o1wc7DJrmC"
      },
      "source": [
        "# Assignment 1:  Linear Regression\r\n",
        "Dataset: monet.csv\r\n",
        "\r\n",
        "Dependent variable: PRICE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpK5d7wJOoQF"
      },
      "source": [
        "# Import primary libraries\r\n",
        "%matplotlib inline\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import numpy as np\r\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mvNMq9kKXRT"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXiAd0vKFM9V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "eb90e026-6b69-417f-a078-06da64e11ef3"
      },
      "source": [
        "# Open and load dataset\r\n",
        "data = pd.read_csv('/content/monet.csv')\r\n",
        "print('data shape: ', data.shape)\r\n",
        "data.head(5)\r\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-1bac5de70ead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Open and load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/monet.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/monet.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJH57HNJKrWf"
      },
      "source": [
        "## Exploratory Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I-26YIvNh4r"
      },
      "source": [
        "### Explore missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbldai-YDw7z"
      },
      "source": [
        "# Investigate missing values\r\n",
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zYehwpCD_GK"
      },
      "source": [
        "It shows that there is no missing values, so we do not need to do any imputing steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isjR_Ei6NtYL"
      },
      "source": [
        "### Discriptive analysis\r\n",
        "By having some discriptive analysis, we could have some sense of how our data vary over each variable in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI7PvH9lPvTy"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NWe3wbVD-WC"
      },
      "source": [
        "# Plot histograms for each variable to see how they vary\r\n",
        "histograms = data.hist(grid=False, figsize=(10, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H30oRCzDtIud"
      },
      "source": [
        "It is clear that values of the WIDTH and the SALE are not nomally distributed. We may think about normalization for these variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wct0uti6NRBD"
      },
      "source": [
        "### Explore the distribution of the dependent variable - PRICE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE5JXIqZGLmU"
      },
      "source": [
        "# Explore the distribution of the dependent variable - PRICE\r\n",
        "data['PRICE'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jBT0Sv_PXHI"
      },
      "source": [
        "sns.distplot(data['PRICE'], bins = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ1SYTDyP4pG"
      },
      "source": [
        "By looking into the shape of how the dependent variable distributes, we can see most of the density falls between three first bins. There may be some outliers from bin 15.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlf-sZRlMGu8"
      },
      "source": [
        "### Create a new variable\r\n",
        "For the simple LR and multivariate LR that we are going to build, we can create a new variable by combining HEIGHT and WIDTH as sizes of pictures. \r\n",
        "SIZE = HEIGHT * WIDTH\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiMC4TuRMbYB"
      },
      "source": [
        "# Create a new variable by combining HEIGHT AND WIDTH\r\n",
        "data['SIZE'] = data['HEIGHT'] * data['WIDTH']\r\n",
        "data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWnjL-8oSlGy"
      },
      "source": [
        "### Select independent variables\r\n",
        "To select potential predictors for the LR models, we can base on how they are correlated with the target variable. We can visualize their correlations in a heatmap or a scatter plot as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iP4Ok0hWVku"
      },
      "source": [
        "# Plot a heatmap with correlation score\r\n",
        "plt.subplots(figsize = (12,8))\r\n",
        "sns.heatmap(data.corr(), annot=True)    # get correlation score matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dfe23G1YLf0"
      },
      "source": [
        "The correlation score is from -1 to 1. The score value that is close to -1 shows a strong negative correlation whereas the score close to 1 indicates a strong positive correlation between two variables. If it is close to 0, the two variables are not correlated.\r\n",
        "\r\n",
        "From the heatmap, it seems that no variables are highly correlated with the dependent variable, PRICE. WIDTH and SIZE are most correlated with the same score (0.35), so either of them could be potential predictors of the models. We could use scatter plots to see their correlations more clearly. Note that HEIGHT and WIDTH certainly have co-linearity with SIZE since HEIGHT and WIDTH were combined to create SIZE. Therefore, we will not input either HEIGHT or WIDTH together with SIZE into the training model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1j0ubIEThDj"
      },
      "source": [
        "# Plot SIZE and PRICE \r\n",
        "sns.lmplot(x= 'SIZE', y = 'PRICE', data = data, ci = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98i2jQRuVJHy"
      },
      "source": [
        "# Plot WIDTH and PRICE\r\n",
        "sns.lmplot(x= 'WIDTH', y = 'PRICE', data = data, ci = None)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP3UR_3cV9G8"
      },
      "source": [
        "# Plot HEIGHT and PRICE\r\n",
        "sns.lmplot(x= 'HEIGHT', y = 'PRICE', data = data, ci = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-C2yXhlGpJM"
      },
      "source": [
        "It looks like PRICE increases along with SIZE or WIDTH or HEIGHT; however, there are not exactly clear lines fitted in the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlZ0upNSYCGz"
      },
      "source": [
        "## Linear Regression Models\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fovv2UHvIh0"
      },
      "source": [
        "X: variables known as independent variables, predictors, features\r\n",
        "\r\n",
        "Y: variables known as dependent or target variable "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa1sbqENsEKi"
      },
      "source": [
        "### Univariate LR Model\r\n",
        "\r\n",
        "Univariate LR or Simple LR models get only one input variable as its single predictor. It has the line fit to data with a form: \r\n",
        "                                  y = ax+b\r\n",
        "\r\n",
        "with a known as coefficient or slope and b as adjustment or intercept."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAjHgFgaGBgW"
      },
      "source": [
        "#### Model 1\r\n",
        "Predictor/Indepedent variable (X): SIZE\r\n",
        "\r\n",
        "Dependent variable (Y): PRICE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RyTnQJ8sm0T"
      },
      "source": [
        "# Split dataset for training (80%) and testing (20%)\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['SIZE']], data['PRICE'], train_size = 0.8)\r\n",
        "\r\n",
        "print ('Shapes of X_train, y_train: ', X_train.shape, y_train.shape)\r\n",
        "print ('Shapes of X_test, y_test: ', X_test.shape, y_test.shape)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTIFChKI11p6"
      },
      "source": [
        "# Build a LR model\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "slr = LinearRegression()\r\n",
        "\r\n",
        "# Fit the model into the training data\r\n",
        "slr.fit (X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7HBw1GJA171"
      },
      "source": [
        "# Apply the model to predict y in the test set\r\n",
        "y_test_pred = slr.predict (X_test) \r\n",
        "\r\n",
        "# Apply the model to predict y in the train set\r\n",
        "y_train_pred = slr.predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFOfjNtpLQIG"
      },
      "source": [
        "# Print coefficient and intercept of the model\r\n",
        "print ('Intercept of the model 1: ', slr.intercept_)\r\n",
        "print ('Coefficient of the model 1: ', slr.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSibQ4FUJdFq"
      },
      "source": [
        "**Model Evaluation**\r\n",
        "\r\n",
        "To evaluate the performance of LR model, we could use Mean Squared Error (MSE) as Cost Function.\r\n",
        "MSE measures how much the model prediction varies from the actual values.\r\n",
        "\r\n",
        "In general, we try to minimize the MSE cost function.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoxxpMy8BwTh"
      },
      "source": [
        "# Evaluate the model performance in the training set\r\n",
        "from sklearn.metrics import mean_squared_error, r2_score\r\n",
        "mse = mean_squared_error (y_train, y_train_pred)\r\n",
        "print('Model 1 - Evaluation on MSE:')\r\n",
        "print ('-'*30)\r\n",
        "print('MSE in the training set: {:.2f}'.format(mse))\r\n",
        "\r\n",
        "\r\n",
        "# Evaluate the model performance in the testing set\r\n",
        "mse = mean_squared_error (y_test, y_test_pred)\r\n",
        "print('\\nMSE in the test set: {:.2f}'.format(mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DenHiXwXa6QX"
      },
      "source": [
        "Obviously, the first model seems to work quite well but its MSE cost function in the train set is still pretty high. We can finetune this model with the goal to decrease its MSE cost function. Looking back the distribution of the dependent variable, we can see it is substantially positively-skewed. For this case, we can try transforming the target variable by applying a logarithmic function for it before training.\r\n",
        "\r\n",
        "The plots below presents how the logarithmic transformation could make the dependent variable less skewed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcOJbIs4jr01"
      },
      "source": [
        "# Plot the original and transformed dependent variable\r\n",
        "\r\n",
        "# original dependent variable\r\n",
        "y = data['PRICE'] \r\n",
        "\r\n",
        "# apply logarithmic function to transform the dependent variable \r\n",
        "y_trans = np.log(y.values.reshape(-1,1))       \r\n",
        "\r\n",
        "f, (ax0, ax1) = plt.subplots(1, 2)\r\n",
        "\r\n",
        "# Plot the original dependent variable\r\n",
        "ax0.hist(y, bins=100)\r\n",
        "ax0.set_ylabel('Density')\r\n",
        "ax0.set_xlabel('Target values')\r\n",
        "ax0.set_title('Target distribution')\r\n",
        "\r\n",
        "# Plot the transformed dependent variable\r\n",
        "ax1.hist(y_trans, bins=100)\r\n",
        "ax1.set_ylabel('Density')\r\n",
        "ax1.set_xlabel('Target values')\r\n",
        "ax1.set_title('Transformed target distribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TawBnyvhHMQU"
      },
      "source": [
        "Similarly, we can try transforming the SIZE independent variable since previously it showed to be positive skewed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhRZmJJYHKvh"
      },
      "source": [
        "# Plot the original and transformed dependent variable\r\n",
        "\r\n",
        "# original independent variable\r\n",
        "x = data['SIZE']\r\n",
        "\r\n",
        "# apply logarithmic function to transform the independent variable \r\n",
        "x_trans = np.log(x.values.reshape(-1,1))      \r\n",
        "\r\n",
        "f, (ax0, ax1) = plt.subplots(1, 2)\r\n",
        "\r\n",
        "# Plot the original dependent variable\r\n",
        "ax0.hist(x, bins=100)\r\n",
        "ax0.set_ylabel('Density')\r\n",
        "ax0.set_xlabel('Predictor values')\r\n",
        "ax0.set_title('Predictor distribution')\r\n",
        "\r\n",
        "# Plot the transformed dependent variable\r\n",
        "ax1.hist(x_trans, bins=100)\r\n",
        "ax1.set_ylabel('Density')\r\n",
        "ax1.set_xlabel('Predictor values')\r\n",
        "ax1.set_title('Transformed predictor distribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zg3qDo3PiAd"
      },
      "source": [
        "# Plot a scatter plot of the transformed SIZE and the transformed PRICE\r\n",
        "sns.regplot(x=x_trans, y=y_trans, ci = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3p5mBc53KZG"
      },
      "source": [
        "The above scatter indicates transformations create a better linear relationship between the two variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkEOnbP5J02u"
      },
      "source": [
        "#### Model 2\r\n",
        "This model is the finetuned by transforming independent and dependent variables of previous model.\r\n",
        "\r\n",
        "X : log(SIZE)\r\n",
        "\r\n",
        "Y: log(PRICE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOp6pfGQm9Je"
      },
      "source": [
        "from sklearn.compose import TransformedTargetRegressor\r\n",
        "\r\n",
        "# Split data for training (80%) and testing (20%)\r\n",
        "X_train_trans, X_test_trans, y_train_trans, y_test_trans = train_test_split(x_trans, y_trans, train_size = 0.8)\r\n",
        "print ('Shape of X_train and y_train: ', X_train.shape, y_train.shape)\r\n",
        "print ('Shape of X_test and y_test: ', X_test.shape, y_test.shape)\r\n",
        "\r\n",
        "# Build a LR model\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "slr_trans = LinearRegression()\r\n",
        "\r\n",
        "# Fit the model into the training data\r\n",
        "slr_trans.fit (X_train_trans, y_train_trans)\r\n",
        "\r\n",
        "# Apply the model to predict y in the test set\r\n",
        "y_test_trans_pred = slr_trans.predict (X_test_trans) \r\n",
        "\r\n",
        "# Apply the model to predict y in the train set\r\n",
        "y_train_trans_pred = slr_trans.predict(X_train_trans)\r\n",
        "\r\n",
        "# Evaluate the model performance in the training set\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "mse_train = mean_squared_error (y_train_trans, y_train_trans_pred)\r\n",
        "\r\n",
        "print ('\\nModel 2 - evaluation on MSE: ')\r\n",
        "print ('-'*30)\r\n",
        "print('MSE in the training set: {:.2f}'.format(mse_train))\r\n",
        "\r\n",
        "# Evaluate the model performance in the testing set\r\n",
        "mse_test = mean_squared_error (y_test_trans, y_test_trans_pred)\r\n",
        "print('\\nMSE in the test set: {:.2f}'.format(mse_test))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZETM74N2OQX-"
      },
      "source": [
        "# Print intercept and coefficient of the model\r\n",
        "print ('Intercept of the model 2: ', slr_trans.intercept_)\r\n",
        "print('Coefficient of the model 2: ', slr_trans.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWtXbzD_Zqit"
      },
      "source": [
        "***Conclusion: Based on MSE of the above simple LR models, we found that the model 2, which has the transformed SIZE and the transformed PRICE as independent and dependent variables, achieved the best performance. ***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa35BHuAbWPl"
      },
      "source": [
        "### Multivariate LR Model\r\n",
        "Multivariate LR has more than one predictors.\r\n",
        "For our given dataset, the variables which are likely predictors are the SIZE,  the HOUSE, and the SIGNED. The HEIGHT and WIDTH variables are multicolinear with SIZE as mentioned previously, and PICTURE contains unique values. Therefore, they will not be used in the model. \r\n",
        "\r\n",
        "The SIGNED and HOUSE variables contain discrete values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KGA6gXumgqx"
      },
      "source": [
        "# Explore the two discrete variables, SIGNED and HOUSE\r\n",
        "var_names = [ 'SIGNED', 'HOUSE ']\r\n",
        "for name in var_names: \r\n",
        "  print(name,'\\n','-'*25)\r\n",
        "  print(data[name].value_counts(),'\\n')\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlMtyEsEnfnY"
      },
      "source": [
        "The SIGNED is a binary variable, but the HOUSE variable has three classes. We still can use discrete variables for LR, but we need to do dummy coding before inputing into the LR model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN7n8RGslhBz"
      },
      "source": [
        "# Create dummy columns of the HOUSE variable\r\n",
        "dummies = pd.get_dummies(data['HOUSE '], prefix='HOUSE')\r\n",
        "\r\n",
        "# Join the dummy columns with the dataset\r\n",
        "data=data.join(dummies)\r\n",
        "display(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCitRcinc5mR"
      },
      "source": [
        "As mentioned previously, since the SIZE independent variable and PRICE dependent variable are positively skewed, we will use their logarithmicaly-transformed values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWwjUgsnqEf9"
      },
      "source": [
        "# Prepare data\r\n",
        "\r\n",
        "# add the transformed SIZE variable into the dataset\r\n",
        "data['SIZE_log']=x_trans\r\n",
        "\r\n",
        "X = data[['SIZE_log', 'SIGNED', 'HOUSE_1', 'HOUSE_2', 'HOUSE_3']]\r\n",
        "y = y_trans\r\n",
        "print('X shape and y shape: ', X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9mgrrYchT-H"
      },
      "source": [
        "# Split data for training and test\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8 )\r\n",
        "\r\n",
        "print ('Shape of X_train and y_train: ', X_train.shape, y_train.shape)\r\n",
        "print ('Shape of X_test and y_test: ', X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPmFNFf0eDaF"
      },
      "source": [
        "# Build a multivariate LR model\r\n",
        "mlr=LinearRegression()\r\n",
        "\r\n",
        "# Fit the built model into training set\r\n",
        "mlr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RTvARn3i_Eg"
      },
      "source": [
        "# Apply the model to predict the PRICE in the test set\r\n",
        "y_test_pred = mlr.predict(X_test)\r\n",
        "\r\n",
        "# Apply the model to predict the PRICE in the training set\r\n",
        "y_train_pred = mlr.predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRiVM977j9Uc"
      },
      "source": [
        "# Evaluate the model performance in the training set\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "mse_train = mean_squared_error (y_train, y_train_pred)\r\n",
        "\r\n",
        "print ('\\nMultivariate LR model - Evaluation on MSE: ')\r\n",
        "print ('-'*40)\r\n",
        "print('MSE in the training set: {:.2f}'.format(mse_train))\r\n",
        "\r\n",
        "# Evaluate the model performance in the testing set\r\n",
        "mse_test = mean_squared_error (y_test, y_test_pred)\r\n",
        "print('\\nMSE in the test set: {:.2f}'.format(mse_test))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StW3ZI_sAL9o"
      },
      "source": [
        "The model performed very well on low MSE scores. However, the model achieved a lower (better) MSE on the test set than on the traning set. This leaves some thoughts about underfitting. However, generally, low MSE indicates good performance of the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRCaHaTDk00j"
      },
      "source": [
        "# Print coefficients and intercepts of the model\r\n",
        "print ('Intercept of the multivariate model: ', mlr.intercept_)\r\n",
        "print('Coefficient of the multivariate model: ', mlr.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}