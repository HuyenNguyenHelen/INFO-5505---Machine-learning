{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HuyenNguyen_Assignment 4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPXHS72qj5Gd+ooFDe4vMMB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/INFO-5505---Machine-learning/blob/main/HuyenNguyen_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_yMppN_P_GC"
      },
      "source": [
        "# Assignment 4: Random Forest\r\n",
        "* Dataset: [Breast Cancer Wisconsin](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)\r\n",
        "* Goals: Given the Breast Cancer Features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass, train a model for predicting/diagnosing whether the observations are maglinant (M) or benign (B)\r\n",
        "*   Dependent variable/ Target variable: **diagnosis**\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v4G3WymSZFw"
      },
      "source": [
        "# Importing essential libraries \r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suY0adZlSPJc"
      },
      "source": [
        "## Loading the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "THqvxiSxTIiR",
        "outputId": "70c2343f-69c4-4a17-862e-7bf812e6e83f"
      },
      "source": [
        "# Loading the dataset\r\n",
        "data=pd.read_csv('/content/data-breastCancer.csv')\r\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302         M  ...                  0.11890          NaN\n",
              "1    842517         M  ...                  0.08902          NaN\n",
              "2  84300903         M  ...                  0.08758          NaN\n",
              "3  84348301         M  ...                  0.17300          NaN\n",
              "4  84358402         M  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6jwB19pjKH7"
      },
      "source": [
        "## Exploring the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA5PxtKNYmnP",
        "outputId": "4b8daf4f-ce4c-49b0-823b-bd2c43fe33c6"
      },
      "source": [
        "# Printing column names\r\n",
        "print('the number of columns: {}'.format(len(data.columns)))\r\n",
        "print('column names: \\n', [name for name in data.columns])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the number of columns: 33\n",
            "column names: \n",
            " ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mPgocuAY1dQ"
      },
      "source": [
        "There are 33 variables including the independent and dependent variables. However, two of them contain no useful information: 'id' and 'Unnamed: 32'. Therefore, we will delete these columns in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "3vkeJ2Xabk_k",
        "outputId": "c2ee55f0-4350-4180-9697-c481002c4b62"
      },
      "source": [
        "# Explore the dataset with some summary statistics\r\n",
        "data.describe()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.690000e+02</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.037183e+07</td>\n",
              "      <td>14.127292</td>\n",
              "      <td>19.289649</td>\n",
              "      <td>91.969033</td>\n",
              "      <td>654.889104</td>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.181162</td>\n",
              "      <td>0.062798</td>\n",
              "      <td>0.405172</td>\n",
              "      <td>1.216853</td>\n",
              "      <td>2.866059</td>\n",
              "      <td>40.337079</td>\n",
              "      <td>0.007041</td>\n",
              "      <td>0.025478</td>\n",
              "      <td>0.031894</td>\n",
              "      <td>0.011796</td>\n",
              "      <td>0.020542</td>\n",
              "      <td>0.003795</td>\n",
              "      <td>16.269190</td>\n",
              "      <td>25.677223</td>\n",
              "      <td>107.261213</td>\n",
              "      <td>880.583128</td>\n",
              "      <td>0.132369</td>\n",
              "      <td>0.254265</td>\n",
              "      <td>0.272188</td>\n",
              "      <td>0.114606</td>\n",
              "      <td>0.290076</td>\n",
              "      <td>0.083946</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.250206e+08</td>\n",
              "      <td>3.524049</td>\n",
              "      <td>4.301036</td>\n",
              "      <td>24.298981</td>\n",
              "      <td>351.914129</td>\n",
              "      <td>0.014064</td>\n",
              "      <td>0.052813</td>\n",
              "      <td>0.079720</td>\n",
              "      <td>0.038803</td>\n",
              "      <td>0.027414</td>\n",
              "      <td>0.007060</td>\n",
              "      <td>0.277313</td>\n",
              "      <td>0.551648</td>\n",
              "      <td>2.021855</td>\n",
              "      <td>45.491006</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.030186</td>\n",
              "      <td>0.006170</td>\n",
              "      <td>0.008266</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>4.833242</td>\n",
              "      <td>6.146258</td>\n",
              "      <td>33.602542</td>\n",
              "      <td>569.356993</td>\n",
              "      <td>0.022832</td>\n",
              "      <td>0.157336</td>\n",
              "      <td>0.208624</td>\n",
              "      <td>0.065732</td>\n",
              "      <td>0.061867</td>\n",
              "      <td>0.018061</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>8.670000e+03</td>\n",
              "      <td>6.981000</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>0.049960</td>\n",
              "      <td>0.111500</td>\n",
              "      <td>0.360200</td>\n",
              "      <td>0.757000</td>\n",
              "      <td>6.802000</td>\n",
              "      <td>0.001713</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007882</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>7.930000</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.055040</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.692180e+05</td>\n",
              "      <td>11.700000</td>\n",
              "      <td>16.170000</td>\n",
              "      <td>75.170000</td>\n",
              "      <td>420.300000</td>\n",
              "      <td>0.086370</td>\n",
              "      <td>0.064920</td>\n",
              "      <td>0.029560</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>0.057700</td>\n",
              "      <td>0.232400</td>\n",
              "      <td>0.833900</td>\n",
              "      <td>1.606000</td>\n",
              "      <td>17.850000</td>\n",
              "      <td>0.005169</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.015090</td>\n",
              "      <td>0.007638</td>\n",
              "      <td>0.015160</td>\n",
              "      <td>0.002248</td>\n",
              "      <td>13.010000</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>84.110000</td>\n",
              "      <td>515.300000</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.147200</td>\n",
              "      <td>0.114500</td>\n",
              "      <td>0.064930</td>\n",
              "      <td>0.250400</td>\n",
              "      <td>0.071460</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>9.060240e+05</td>\n",
              "      <td>13.370000</td>\n",
              "      <td>18.840000</td>\n",
              "      <td>86.240000</td>\n",
              "      <td>551.100000</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.092630</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.324200</td>\n",
              "      <td>1.108000</td>\n",
              "      <td>2.287000</td>\n",
              "      <td>24.530000</td>\n",
              "      <td>0.006380</td>\n",
              "      <td>0.020450</td>\n",
              "      <td>0.025890</td>\n",
              "      <td>0.010930</td>\n",
              "      <td>0.018730</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>14.970000</td>\n",
              "      <td>25.410000</td>\n",
              "      <td>97.660000</td>\n",
              "      <td>686.500000</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.226700</td>\n",
              "      <td>0.099930</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.080040</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.813129e+06</td>\n",
              "      <td>15.780000</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>104.100000</td>\n",
              "      <td>782.700000</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>0.066120</td>\n",
              "      <td>0.478900</td>\n",
              "      <td>1.474000</td>\n",
              "      <td>3.357000</td>\n",
              "      <td>45.190000</td>\n",
              "      <td>0.008146</td>\n",
              "      <td>0.032450</td>\n",
              "      <td>0.042050</td>\n",
              "      <td>0.014710</td>\n",
              "      <td>0.023480</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>18.790000</td>\n",
              "      <td>29.720000</td>\n",
              "      <td>125.400000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.339100</td>\n",
              "      <td>0.382900</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.092080</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.113205e+08</td>\n",
              "      <td>28.110000</td>\n",
              "      <td>39.280000</td>\n",
              "      <td>188.500000</td>\n",
              "      <td>2501.000000</td>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.426800</td>\n",
              "      <td>0.201200</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>0.097440</td>\n",
              "      <td>2.873000</td>\n",
              "      <td>4.885000</td>\n",
              "      <td>21.980000</td>\n",
              "      <td>542.200000</td>\n",
              "      <td>0.031130</td>\n",
              "      <td>0.135400</td>\n",
              "      <td>0.396000</td>\n",
              "      <td>0.052790</td>\n",
              "      <td>0.078950</td>\n",
              "      <td>0.029840</td>\n",
              "      <td>36.040000</td>\n",
              "      <td>49.540000</td>\n",
              "      <td>251.200000</td>\n",
              "      <td>4254.000000</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>1.058000</td>\n",
              "      <td>1.252000</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.663800</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  radius_mean  ...  fractal_dimension_worst  Unnamed: 32\n",
              "count  5.690000e+02   569.000000  ...               569.000000          0.0\n",
              "mean   3.037183e+07    14.127292  ...                 0.083946          NaN\n",
              "std    1.250206e+08     3.524049  ...                 0.018061          NaN\n",
              "min    8.670000e+03     6.981000  ...                 0.055040          NaN\n",
              "25%    8.692180e+05    11.700000  ...                 0.071460          NaN\n",
              "50%    9.060240e+05    13.370000  ...                 0.080040          NaN\n",
              "75%    8.813129e+06    15.780000  ...                 0.092080          NaN\n",
              "max    9.113205e+08    28.110000  ...                 0.207500          NaN\n",
              "\n",
              "[8 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs-w788QdFae"
      },
      "source": [
        "The min, max, mean, and standard deviation from the above summary statistics of the dataset indicate that our data have different scales or ranges. However, fortunately, decision tree or random forest algorithms can handle well this issue because they made local optimums, unlike logistic regression or others. Therefore, we do not have to normalize our data, for maintaining the model's interpretability. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgKSAJIOS7jq"
      },
      "source": [
        "### Exploring missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOE7WAcvajZp",
        "outputId": "b3d21642-1826-4db5-f0b5-4ecafdf52e67"
      },
      "source": [
        "# Checking missing values in the dataset\r\n",
        "data.isnull().sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                           0\n",
              "diagnosis                    0\n",
              "radius_mean                  0\n",
              "texture_mean                 0\n",
              "perimeter_mean               0\n",
              "area_mean                    0\n",
              "smoothness_mean              0\n",
              "compactness_mean             0\n",
              "concavity_mean               0\n",
              "concave points_mean          0\n",
              "symmetry_mean                0\n",
              "fractal_dimension_mean       0\n",
              "radius_se                    0\n",
              "texture_se                   0\n",
              "perimeter_se                 0\n",
              "area_se                      0\n",
              "smoothness_se                0\n",
              "compactness_se               0\n",
              "concavity_se                 0\n",
              "concave points_se            0\n",
              "symmetry_se                  0\n",
              "fractal_dimension_se         0\n",
              "radius_worst                 0\n",
              "texture_worst                0\n",
              "perimeter_worst              0\n",
              "area_worst                   0\n",
              "smoothness_worst             0\n",
              "compactness_worst            0\n",
              "concavity_worst              0\n",
              "concave points_worst         0\n",
              "symmetry_worst               0\n",
              "fractal_dimension_worst      0\n",
              "Unnamed: 32                569\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULJ7Z-qMd8Gm"
      },
      "source": [
        "As we can see, there is no missing values, so we do not need to do data cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uok_QjEnTXgB"
      },
      "source": [
        "### Exploring the data distribution of the dependent variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9J2RI7ojU-L",
        "outputId": "4258f498-9e6c-493e-89b7-282519623768"
      },
      "source": [
        "# Explore data distribution on the dependent variable \r\n",
        "print( 'class distribution in the dependent variable: \\n', data['diagnosis'].value_counts())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class distribution in the dependent variable: \n",
            " B    357\n",
            "M    212\n",
            "Name: diagnosis, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "QCNEvWoPZsJX",
        "outputId": "af99169e-ac86-4ed7-be73-31a4af401e01"
      },
      "source": [
        "sns.countplot(x='diagnosis', data=data)\r\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DflcvXI8Uq_q"
      },
      "source": [
        "The dependent variable \"diagnosis\" contains binary values: M (malignant) and B (benight). The proportion of the two classes are imbalanced as shown from the above bar chart. Although the imbalance is not much, we should keep this in mind when we train and validate the model later, ensuring our models have aquadetely learned and tested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dGEXSmxdN5-"
      },
      "source": [
        "### Preprocessing\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW1oVTNisMQb"
      },
      "source": [
        "* Deleting the columns contain no information: \"id\" and \"Unnamed: 32\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyYMcu6vb1CS",
        "outputId": "b535daef-9a3f-4374-a4bc-f160039f0d94"
      },
      "source": [
        "# Deleting the columns contain no information\r\n",
        "del data['id']\r\n",
        "del data ['Unnamed: 32']\r\n",
        "\r\n",
        "# Printing the numer of columns after deleting id columns\r\n",
        "print('the number of columns: ', len(data.columns))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the number of columns:  31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeA6b9_RsF0k"
      },
      "source": [
        "* Convert discrete data into numeric. Here, only the dependent variable contain discrete values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "18EzjauFdlHA",
        "outputId": "93b422c5-c1df-43a6-a651-6bc5def50b48"
      },
      "source": [
        "data['diagnosis']=np.where(data['diagnosis']=='M', 1, 0)\r\n",
        "data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   diagnosis  radius_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "0          1        17.99  ...          0.4601                  0.11890\n",
              "1          1        20.57  ...          0.2750                  0.08902\n",
              "2          1        19.69  ...          0.3613                  0.08758\n",
              "3          1        11.42  ...          0.6638                  0.17300\n",
              "4          1        20.29  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQknD5EzdDs1"
      },
      "source": [
        "## Developing Random Forest models\r\n",
        "Random forest algorithm overcomes limitations of Decision tree that is sensitive to any minor changes in the input data and tend to overfit because it maximize local optimum, performing poorly in unseen data. Random forest also solves problems of Bagging (Boostrapping Aggregation) Decision tree that uses all features for spliting creation, creating complexity of the tree.\r\n",
        "Random forest not only  generates samples with bootstrapping method like Bagging, but also take random collection of features for each subset.\r\n",
        "\r\n",
        "Here we will create a base model with Random Forest in sklearn. Then we will experiment a variety of parameters to select the best parameters for tuning the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxpCOnxQi9Av"
      },
      "source": [
        "### Spliting the data into training and testing sets\r\n",
        "Here we will split the data into two parts: training set (80%), and test set (20%). \r\n",
        "\r\n",
        "For the training set (80%), we will then use k-fold cross validation method for validating our models. By doing so, the training set will be splitted into k-folds (i.e., we decide to divide into 10 folds). For each iteration, 9 folds will be used for training and 1 set for validating . \r\n",
        "\r\n",
        "We will not use the test set (20%) until we have finished training, and model selection. We only use the test set for evaluating our selected model to ensure having the most reliable model evaluation result before deploying the model.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY81vgPTi5gt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a0ee7c-11f8-4b76-cd48-67822b3b8dca"
      },
      "source": [
        "# Spliting the data into training (80%) and testing (20%) sets\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X = data.drop('diagnosis', axis=1)\r\n",
        "y = data['diagnosis']\r\n",
        "X_train, X_test, y_train, y_test = train_test_split (X, y,train_size = 0.8)\r\n",
        "print ('Shapes of X_train, y_train: ', X_train.shape, y_train.shape)\r\n",
        "print ('Shapes of X_test, y_test: ', X_test.shape, y_test.shape)\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shapes of X_train, y_train:  (455, 30) (455,)\n",
            "Shapes of X_test, y_test:  (114, 30) (114,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK488A8ORL5z"
      },
      "source": [
        "### Based Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgo9TUt3dAyS"
      },
      "source": [
        "# Building a Random forest model\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "base_rf = RandomForestClassifier (random_state = 42)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGr1wDBTtOWe",
        "outputId": "56a81964-0c8d-42e6-9134-a892743996c3"
      },
      "source": [
        "# Validate the model's performance using k-fold cross validation\r\n",
        "from sklearn.model_selection import cross_validate\r\n",
        "cv = cross_validate (base_rf, X_train, y_train, cv = 10)\r\n",
        "print(\"Base model's accuracy score of 10-fold cross validation:\\n\", cv['test_score'])\r\n",
        "print(\"Base model's cross validation accuracy mean score: \\n\", cv['test_score'].mean())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base model's accuracy score of 10-fold cross validation:\n",
            " [0.93478261 0.89130435 0.95652174 0.93478261 0.93478261 0.97777778\n",
            " 0.97777778 0.95555556 0.97777778 1.        ]\n",
            "Base model's cross validation accuracy mean score: \n",
            " 0.9541062801932367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYqtnuqZSg4J"
      },
      "source": [
        "The k-fold validation allows us to get a generalized estimate performance score of the model. The base model performed quite well, with an accuracy score of 0.954 on the training set. However, we still hope to improve the performance by tuning the hyperparameters. \r\n",
        "\r\n",
        "Let's see parameters used by the base model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wHy2MSwFF7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55523f5d-7bae-4bc9-bb14-dd7eb95ceb82"
      },
      "source": [
        "# Checking current parameters of the base model\r\n",
        "print(\"Base model's parameters:\\n\", base_rf.get_params())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base model's parameters:\n",
            " {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPtJVhkde_wG"
      },
      "source": [
        "### Tunning parameters\r\n",
        "We can use RandomizedSearchCV or GridSearchCV with Cross Validation for searching the best parameters. However, RandomizedSearchCV is more suitable when we have not determined any certain parameters, except for boostrapping. \r\n",
        "\r\n",
        "Below are parameters that are most important for us to tune:\r\n",
        "\r\n",
        "\r\n",
        "*   'bootstrap': randomly sampling with replacement. We expect to use this method to generate resample of the training data for building trees\r\n",
        "*   'max_depth': the maximum depth of the tree. It is true that increasing max_depth can improve accuracy of the model on the training set, but the model perform poorly in the unseen data. To avoid the overfitting problem of the Decision Tree that is a fully-grown tree, we should limit this parameter.\r\n",
        "*   'max_features': for setting the number of features to consider when looking for the best split\r\n",
        "*   'min_sample_leaf': The minimum number of samples required to be at a leaf node. We will not want one or two sample at each leaf node.\r\n",
        "*   'min_samples_split':The minimum number of samples required to split an internal node\r\n",
        "*   'n_estimators': The number of trees in the forest. Note that it is not true that more trees the model has, the better the model performs.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gWCzalNGLSy"
      },
      "source": [
        "#### Searching the best parameters for tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkxDrfIde3hB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7b3b07-a1d5-41b3-cc09-a379eb5a8622"
      },
      "source": [
        "# Tunning parameters\r\n",
        "\r\n",
        "# Using randomized search on hyperparameters\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "\r\n",
        "# Setting a grid of parameters to sample \r\n",
        "# Setting the number of trees in random forest classifiers\r\n",
        "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10, endpoint = True)]\r\n",
        "# Setting the number of features for each tree\r\n",
        "max_features = ['auto', 'sqrt', 'log2']\r\n",
        "# Setting the number of depth levels for each tree\r\n",
        "max_depth = [int(x) for x in np.linspace(start = 3, stop = 36, num=33, endpoint = True)]\r\n",
        "# Setting the minimum number of samples required to split an internal node\r\n",
        "min_samples_split = [5, 10, 15]\r\n",
        "# Setting the minimum number of samples required to be at a leaf node \r\n",
        "min_samples_leaf = [3, 4, 5]\r\n",
        "# Select bootstrap method for building trees \r\n",
        "bootstrap = ['True']\r\n",
        "\r\n",
        "# Create the random grid\r\n",
        "random_grid = {'n_estimators':n_estimators, 'max_features': max_features, 'max_depth': max_depth, 'min_samples_split': min_samples_split,'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\r\n",
        "print(random_grid)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36], 'min_samples_split': [5, 10, 15], 'min_samples_leaf': [3, 4, 5], 'bootstrap': ['True']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o57zWmwiCdP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e336816-247e-4b3c-fbf0-282e45129e84"
      },
      "source": [
        "# Create a base randomforest model for tuning\r\n",
        "tune_base_rf = RandomForestClassifier(random_state=42)\r\n",
        "\r\n",
        "# Create a randomized search cross validation model for searching for the best hyperparameters for the base rf model over 100 parameters combination\r\n",
        "random_search_rf = RandomizedSearchCV (estimator=tune_base_rf, param_distributions  = random_grid, random_state=42, cv = 10, n_iter=100)\r\n",
        "\r\n",
        "# fit the randomized search CV model into the training set\r\n",
        "random_search_rf.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Print the best parameters\r\n",
        "random_search_rf.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': 'True',\n",
              " 'max_depth': 11,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_leaf': 3,\n",
              " 'min_samples_split': 5,\n",
              " 'n_estimators': 300}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHjO3BL-S1PZ"
      },
      "source": [
        "Using the cross validation randomized search method, we found the best parameters for our tuned random forest model. Hopefully, we could achieve a bit higher performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vjgI50oGC9i"
      },
      "source": [
        "### The tuned random forest model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3HHL26EGfXT",
        "outputId": "13d89848-483c-49d9-c04d-ff68db9582f9"
      },
      "source": [
        "# Creating a tuned random forest model with the best parameters choosen by  the cv randomized search algorithm\r\n",
        "tuned_rf = RandomForestClassifier (n_estimators = 300, min_samples_split = 5, min_samples_leaf = 3, max_features = 'sqrt',  max_depth = 11, bootstrap = True, random_state=42)\r\n",
        "\r\n",
        "# Validating the model using k-fold cross validation (k=10)\r\n",
        "tuned_cv = cross_validate (tuned_rf, X_train, y_train, cv = 10)\r\n",
        "print(\"The tuned model's accuracy scores in 5-fold cross validation:\\n\", tuned_cv['test_score'])\r\n",
        "print(\"The final base tuned model's cv accuracy score: \\n\", tuned_cv['test_score'].mean())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tuned model's accuracy scores in 5-fold cross validation:\n",
            " [0.93478261 0.93478261 0.95652174 0.93478261 0.93478261 0.97777778\n",
            " 0.95555556 0.93333333 0.95555556 1.        ]\n",
            "The final base tuned model's cv accuracy score: \n",
            " 0.9517874396135266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUdiWgz-Za8C"
      },
      "source": [
        "The tuned model did not show to have better performance compared to the base model. That makes sense because we decrease the maximum depths of trees in the tuned model. However, we hope to use those hyperparameters for avoiding the overfitting or high variance of the model on the unseen data. Therefore, we decide to go with the tuned model as our final model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igaqaxhxr4kj"
      },
      "source": [
        "### Evaluating the selected model\r\n",
        "We aims to evaluate the model by using three popular evaluation methods: accuracy score (implying the error rate), confusion matrix, and ROC Curve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri5aECaE20O6"
      },
      "source": [
        "#### With accuracy *score*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOc8cq78AtRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad4b54d-59a2-4018-f4ca-388d36555a70"
      },
      "source": [
        "# Fit the selected model to the training set\r\n",
        "tuned_rf.fit(X_train, y_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=11, max_features='sqrt',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=3, min_samples_split=5,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
              "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq_Oj4mUA0zh"
      },
      "source": [
        "Noted that previously we build a model, then train and validate the model with k-fold cross validation. We did not fit the model into the whole training set before applying k-fold cross validation because we tried to prevent data leakage. Otherwise, the model was trained on the whole training set, so when validating with k-fold cross validation, there were no unseen data to validate the model, and that would result a biased estimate score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S9xtqlJWFBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3636c6-0979-4a3d-984a-249936aa5837"
      },
      "source": [
        "# Applying the selected model to make prediction on the test set\r\n",
        "pred = tuned_rf.predict(X_test)\r\n",
        "\r\n",
        "# Observing the estimate probability of classess in the test set\r\n",
        "pred_prob = tuned_rf.predict_proba (X_test)\r\n",
        "print ('class_0','\\t', 'class_1')\r\n",
        "print(pred_prob[:10])\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class_0 \t class_1\n",
            "[[0.00222222 0.99777778]\n",
            " [0.30420767 0.69579233]\n",
            " [0.84038757 0.15961243]\n",
            " [0.99286111 0.00713889]\n",
            " [0.00848413 0.99151587]\n",
            " [0.86812037 0.13187963]\n",
            " [1.         0.        ]\n",
            " [0.76004131 0.23995869]\n",
            " [0.98442063 0.01557937]\n",
            " [0.0335754  0.9664246 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT8Pc2WW79ma"
      },
      "source": [
        "The result of the estimate probabilities show how classes were assigned. It is a nx2 array. The first column (dimension) is for class 0, and another for the class 1. In the first instance, it was assigned class 1 because it got a probability of almost 1 for this class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psAItIyVj57U",
        "outputId": "28cab59b-07c9-4b29-9da9-ff4b8d8cf919"
      },
      "source": [
        "# Evaluate the selected model on the test set with acurracy scores\r\n",
        "print('Accuracy of the selected model in the test set: {:.4f}'.format(tuned_rf.score(X_test, y_test)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the selected model in the test set: 0.9649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1Rry_Q_DI_p"
      },
      "source": [
        "We got a very high accuracy of the model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLjy1lDn3Fcb"
      },
      "source": [
        "#### With confusion matrix\r\n",
        "We were aware of the imbalanced proportions of the two classes in previous section. Therefore, accuracy score may not make sense. Note that the Malignant (class 1) acounts a lower portion in the dataset. Even if True Positive is high, and False Negative is low, the cost of False Negative is quite high. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EescqEUnBCqO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "9e1c6f75-5048-4681-971c-086f08913eab"
      },
      "source": [
        "# Evaluate the selected model on the test set  with confusion matrix\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "conf_matrix = confusion_matrix(y_test, pred )\r\n",
        "# visualizing confusion matrix\r\n",
        "sns.heatmap(conf_matrix, annot = True)\r\n",
        "plt.xlabel ('predicted values')\r\n",
        "plt.ylabel ('actual values')\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'actual values')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZdQMHsDHskT"
      },
      "source": [
        "From the above confusion matrix, we can see among 104 instances in test set, our model correctly predict 73 Benign and 37 Malignant instances. It only incorrectly predict 3 instances which are actually Benign (FP), and 1 instance that is actually Maglignant (FN). Lower FN indicates that our model satisfy the cost requirement as mentioned previously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpQfnpw917AJ"
      },
      "source": [
        "#### With the ROC curve\r\n",
        "The Receiver Operating Characteristic (ROC) Curve is drew by plotting the true positive rate (TPR) against the false positive rate (FPR). The ROC Curve is a probability curve, telling us how much the model able to distinguish between classes, shown by the area under the curve (AUC). Higher the AUC is, the better the model performs. The AUC is expected to greater than 0.5, or over left-top part compared to the baseline.\r\n",
        "\r\n",
        "*   TPR = TP/P = TP/(TP+FN)\r\n",
        "*   FPR = FP/N = FP/ (FP+TN)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvqaTtOrBpZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c50159b0-930a-45c4-f39a-3651d95c3686"
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\r\n",
        "# Taking the probability of the class 1 (Malignant) on the test set\r\n",
        "pred_prob_c1 = pred_prob[:,1]\r\n",
        "\r\n",
        "# Getting True Positive Rate (tpr) and False Positive Rate (fpr)\r\n",
        "fpr, tpr, threshold = roc_curve (y_test,pred_prob_c1, pos_label = 1)\r\n",
        "\r\n",
        "# Computing Area Under the ROC Curve (roc_auc)\r\n",
        "roc_auc_score = roc_auc_score (y_test,pred_prob_c1)\r\n",
        "roc_auc_score"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9963624338624338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "uQeS22WmQp6U",
        "outputId": "de6d7ea6-1b62-4000-e8fe-2981e745a34a"
      },
      "source": [
        "# Visualizing the ROC Curve\r\n",
        "plt.plot( fpr, tpr)\r\n",
        "plt.plot([0,1], '--')\r\n",
        "plt.xlabel('False Positive Rate')\r\n",
        "plt.ylabel('True Positive Rate')\r\n",
        "plt.title (\"ROC Curve (AUC = {:.3f})\".format(roc_auc_score))\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSIK_JYHq8y8"
      },
      "source": [
        "We can see our AUC is very high, very close to 1 which is the ideal point (1.0). That indicates our model has a very good measure of separability between classes."
      ]
    }
  ]
}