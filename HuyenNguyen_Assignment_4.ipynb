{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HuyenNguyen_Assignment 4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOqetG8vn7ttXN4evACj/NR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/INFO-5505---Machine-learning/blob/main/HuyenNguyen_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_yMppN_P_GC"
      },
      "source": [
        "# Assignment 4: Random Forest\r\n",
        "* Dataset: [Breast Cancer Wisconsin](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)\r\n",
        "* Goals: Given the Breast Cancer Features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass, train a model for predicting/diagnosing whether the observations are maglinant (M) or benign (B)\r\n",
        "*   Dependent variable/ Target variable: **diagnosis**\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v4G3WymSZFw"
      },
      "source": [
        "# Importing essential libraries \r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suY0adZlSPJc"
      },
      "source": [
        "## Loading the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "THqvxiSxTIiR",
        "outputId": "cd513ac7-ad53-4b45-846d-302352294428"
      },
      "source": [
        "# Loading the dataset\r\n",
        "data=pd.read_csv('/content/data-breastCancer.csv')\r\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302         M  ...                  0.11890          NaN\n",
              "1    842517         M  ...                  0.08902          NaN\n",
              "2  84300903         M  ...                  0.08758          NaN\n",
              "3  84348301         M  ...                  0.17300          NaN\n",
              "4  84358402         M  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6jwB19pjKH7"
      },
      "source": [
        "## Exploring the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA5PxtKNYmnP",
        "outputId": "6a7c2fdb-f91f-46d1-c379-e8d872a23898"
      },
      "source": [
        "# Printing column names\r\n",
        "print('the number of columns: {}'.format(len(data.columns)))\r\n",
        "print('column names: \\n', [name for name in data.columns])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the number of columns: 33\n",
            "column names: \n",
            " ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mPgocuAY1dQ"
      },
      "source": [
        "There are 33 variables including the independent and dependent variables. However, two of them contain no useful information: 'id' and 'Unnamed: 32'. Therefore, we will delete these columns in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "3vkeJ2Xabk_k",
        "outputId": "93f9e66f-c75f-44b0-dd2d-4511d15c5d35"
      },
      "source": [
        "# Explore the dataset with some summary statistics\r\n",
        "data.describe()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.690000e+02</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.037183e+07</td>\n",
              "      <td>14.127292</td>\n",
              "      <td>19.289649</td>\n",
              "      <td>91.969033</td>\n",
              "      <td>654.889104</td>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.181162</td>\n",
              "      <td>0.062798</td>\n",
              "      <td>0.405172</td>\n",
              "      <td>1.216853</td>\n",
              "      <td>2.866059</td>\n",
              "      <td>40.337079</td>\n",
              "      <td>0.007041</td>\n",
              "      <td>0.025478</td>\n",
              "      <td>0.031894</td>\n",
              "      <td>0.011796</td>\n",
              "      <td>0.020542</td>\n",
              "      <td>0.003795</td>\n",
              "      <td>16.269190</td>\n",
              "      <td>25.677223</td>\n",
              "      <td>107.261213</td>\n",
              "      <td>880.583128</td>\n",
              "      <td>0.132369</td>\n",
              "      <td>0.254265</td>\n",
              "      <td>0.272188</td>\n",
              "      <td>0.114606</td>\n",
              "      <td>0.290076</td>\n",
              "      <td>0.083946</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.250206e+08</td>\n",
              "      <td>3.524049</td>\n",
              "      <td>4.301036</td>\n",
              "      <td>24.298981</td>\n",
              "      <td>351.914129</td>\n",
              "      <td>0.014064</td>\n",
              "      <td>0.052813</td>\n",
              "      <td>0.079720</td>\n",
              "      <td>0.038803</td>\n",
              "      <td>0.027414</td>\n",
              "      <td>0.007060</td>\n",
              "      <td>0.277313</td>\n",
              "      <td>0.551648</td>\n",
              "      <td>2.021855</td>\n",
              "      <td>45.491006</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.030186</td>\n",
              "      <td>0.006170</td>\n",
              "      <td>0.008266</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>4.833242</td>\n",
              "      <td>6.146258</td>\n",
              "      <td>33.602542</td>\n",
              "      <td>569.356993</td>\n",
              "      <td>0.022832</td>\n",
              "      <td>0.157336</td>\n",
              "      <td>0.208624</td>\n",
              "      <td>0.065732</td>\n",
              "      <td>0.061867</td>\n",
              "      <td>0.018061</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>8.670000e+03</td>\n",
              "      <td>6.981000</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>0.049960</td>\n",
              "      <td>0.111500</td>\n",
              "      <td>0.360200</td>\n",
              "      <td>0.757000</td>\n",
              "      <td>6.802000</td>\n",
              "      <td>0.001713</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007882</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>7.930000</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.055040</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.692180e+05</td>\n",
              "      <td>11.700000</td>\n",
              "      <td>16.170000</td>\n",
              "      <td>75.170000</td>\n",
              "      <td>420.300000</td>\n",
              "      <td>0.086370</td>\n",
              "      <td>0.064920</td>\n",
              "      <td>0.029560</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>0.057700</td>\n",
              "      <td>0.232400</td>\n",
              "      <td>0.833900</td>\n",
              "      <td>1.606000</td>\n",
              "      <td>17.850000</td>\n",
              "      <td>0.005169</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.015090</td>\n",
              "      <td>0.007638</td>\n",
              "      <td>0.015160</td>\n",
              "      <td>0.002248</td>\n",
              "      <td>13.010000</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>84.110000</td>\n",
              "      <td>515.300000</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.147200</td>\n",
              "      <td>0.114500</td>\n",
              "      <td>0.064930</td>\n",
              "      <td>0.250400</td>\n",
              "      <td>0.071460</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>9.060240e+05</td>\n",
              "      <td>13.370000</td>\n",
              "      <td>18.840000</td>\n",
              "      <td>86.240000</td>\n",
              "      <td>551.100000</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.092630</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.324200</td>\n",
              "      <td>1.108000</td>\n",
              "      <td>2.287000</td>\n",
              "      <td>24.530000</td>\n",
              "      <td>0.006380</td>\n",
              "      <td>0.020450</td>\n",
              "      <td>0.025890</td>\n",
              "      <td>0.010930</td>\n",
              "      <td>0.018730</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>14.970000</td>\n",
              "      <td>25.410000</td>\n",
              "      <td>97.660000</td>\n",
              "      <td>686.500000</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.226700</td>\n",
              "      <td>0.099930</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.080040</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.813129e+06</td>\n",
              "      <td>15.780000</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>104.100000</td>\n",
              "      <td>782.700000</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>0.066120</td>\n",
              "      <td>0.478900</td>\n",
              "      <td>1.474000</td>\n",
              "      <td>3.357000</td>\n",
              "      <td>45.190000</td>\n",
              "      <td>0.008146</td>\n",
              "      <td>0.032450</td>\n",
              "      <td>0.042050</td>\n",
              "      <td>0.014710</td>\n",
              "      <td>0.023480</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>18.790000</td>\n",
              "      <td>29.720000</td>\n",
              "      <td>125.400000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.339100</td>\n",
              "      <td>0.382900</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.092080</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.113205e+08</td>\n",
              "      <td>28.110000</td>\n",
              "      <td>39.280000</td>\n",
              "      <td>188.500000</td>\n",
              "      <td>2501.000000</td>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.426800</td>\n",
              "      <td>0.201200</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>0.097440</td>\n",
              "      <td>2.873000</td>\n",
              "      <td>4.885000</td>\n",
              "      <td>21.980000</td>\n",
              "      <td>542.200000</td>\n",
              "      <td>0.031130</td>\n",
              "      <td>0.135400</td>\n",
              "      <td>0.396000</td>\n",
              "      <td>0.052790</td>\n",
              "      <td>0.078950</td>\n",
              "      <td>0.029840</td>\n",
              "      <td>36.040000</td>\n",
              "      <td>49.540000</td>\n",
              "      <td>251.200000</td>\n",
              "      <td>4254.000000</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>1.058000</td>\n",
              "      <td>1.252000</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.663800</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  radius_mean  ...  fractal_dimension_worst  Unnamed: 32\n",
              "count  5.690000e+02   569.000000  ...               569.000000          0.0\n",
              "mean   3.037183e+07    14.127292  ...                 0.083946          NaN\n",
              "std    1.250206e+08     3.524049  ...                 0.018061          NaN\n",
              "min    8.670000e+03     6.981000  ...                 0.055040          NaN\n",
              "25%    8.692180e+05    11.700000  ...                 0.071460          NaN\n",
              "50%    9.060240e+05    13.370000  ...                 0.080040          NaN\n",
              "75%    8.813129e+06    15.780000  ...                 0.092080          NaN\n",
              "max    9.113205e+08    28.110000  ...                 0.207500          NaN\n",
              "\n",
              "[8 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs-w788QdFae"
      },
      "source": [
        "The min, max, mean, and standard deviation from the above summary statistics of the dataset indicate that our data have different scales or ranges. However, fortunately, decision tree or random forest algorithms can handle well this issue because they made local optimums, unlike logistic regression or others. Therefore, we do not have to normalize our data, for maintaining the model's interpretability. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgKSAJIOS7jq"
      },
      "source": [
        "### Exploring missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOE7WAcvajZp",
        "outputId": "05c103dc-9483-4e8f-e7ae-0faef74d85d9"
      },
      "source": [
        "# Checking missing values in the dataset\r\n",
        "data.isnull().sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                           0\n",
              "diagnosis                    0\n",
              "radius_mean                  0\n",
              "texture_mean                 0\n",
              "perimeter_mean               0\n",
              "area_mean                    0\n",
              "smoothness_mean              0\n",
              "compactness_mean             0\n",
              "concavity_mean               0\n",
              "concave points_mean          0\n",
              "symmetry_mean                0\n",
              "fractal_dimension_mean       0\n",
              "radius_se                    0\n",
              "texture_se                   0\n",
              "perimeter_se                 0\n",
              "area_se                      0\n",
              "smoothness_se                0\n",
              "compactness_se               0\n",
              "concavity_se                 0\n",
              "concave points_se            0\n",
              "symmetry_se                  0\n",
              "fractal_dimension_se         0\n",
              "radius_worst                 0\n",
              "texture_worst                0\n",
              "perimeter_worst              0\n",
              "area_worst                   0\n",
              "smoothness_worst             0\n",
              "compactness_worst            0\n",
              "concavity_worst              0\n",
              "concave points_worst         0\n",
              "symmetry_worst               0\n",
              "fractal_dimension_worst      0\n",
              "Unnamed: 32                569\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULJ7Z-qMd8Gm"
      },
      "source": [
        "As we can see, there is no missing values, so we do not need to do data cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uok_QjEnTXgB"
      },
      "source": [
        "### Exploring the data distribution of the dependent variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9J2RI7ojU-L",
        "outputId": "7a9c5f18-9d22-4a96-ae6a-8c2da3002afa"
      },
      "source": [
        "# Explore data distribution on the dependent variable \r\n",
        "print( 'class distribution in the dependent variable: \\n', data['diagnosis'].value_counts())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class distribution in the dependent variable: \n",
            " B    357\n",
            "M    212\n",
            "Name: diagnosis, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "QCNEvWoPZsJX",
        "outputId": "634937c7-76ee-4db9-bd15-b701c4e2f1d2"
      },
      "source": [
        "sns.countplot(x='diagnosis', data=data)\r\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DflcvXI8Uq_q"
      },
      "source": [
        "The dependent variable \"diagnosis\" contains binary values: M (malignant) and B (benight). The proportion of the two classes are imbalanced as shown from the above bar chart. Although the imbalance is not much, we should keep this in mind when we train and validate the model later, ensuring our models have aquadetely learned and tested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dGEXSmxdN5-"
      },
      "source": [
        "### Preprocessing\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW1oVTNisMQb"
      },
      "source": [
        "* Deleting the columns contain no information: \"id\" and \"Unnamed: 32\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyYMcu6vb1CS",
        "outputId": "c8b29613-5667-45a1-a71b-da1f67c53fec"
      },
      "source": [
        "# Deleting the columns contain no information\r\n",
        "del data['id']\r\n",
        "del data ['Unnamed: 32']\r\n",
        "\r\n",
        "# Printing the numer of columns after deleting id columns\r\n",
        "print('the number of columns: ', len(data.columns))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the number of columns:  31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeA6b9_RsF0k"
      },
      "source": [
        "* Convert discrete data into numeric. Here, only the dependent variable contain discrete values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "18EzjauFdlHA",
        "outputId": "a1c5dc0e-152e-4970-83e7-d091bf09fe31"
      },
      "source": [
        "data['diagnosis']=np.where(data['diagnosis']=='M', 1, 0)\r\n",
        "data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   diagnosis  radius_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "0          1        17.99  ...          0.4601                  0.11890\n",
              "1          1        20.57  ...          0.2750                  0.08902\n",
              "2          1        19.69  ...          0.3613                  0.08758\n",
              "3          1        11.42  ...          0.6638                  0.17300\n",
              "4          1        20.29  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQknD5EzdDs1"
      },
      "source": [
        "## Developing Random Forest models\r\n",
        "Random forest algorithm overcomes limitations of Decision tree that is sensitive to any minor changes in the input data and tend to overfit because it maximize local optimum, performing poorly in unseen data. Random forest also solves problems of Bagging (Boostrapping Aggregation) Decision tree that uses all features for spliting creation, creating complexity of the tree.\r\n",
        "Random forest not only  generates samples with bootstrapping method like Bagging, but also take random collection of features for each subset.\r\n",
        "\r\n",
        "Here we will create a base model with Random Forest in sklearn. Then we will experiment a variety of parameters to select the best parameters for tuning the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxpCOnxQi9Av"
      },
      "source": [
        "### Spliting the data into training and testing sets\r\n",
        "Here we will split the data into two parts: training set (80%), and test set (20%). \r\n",
        "\r\n",
        "For the training set (80%), we will then use k-fold cross validation method for validating our models. By doing so, the training set will be splitted into k-folds (i.e., we decide to divide into 10 folds). For each iteration, 9 folds will be used for training and 1 set for validating . \r\n",
        "\r\n",
        "We will not use the test set (20%) until we have finished training, and model selection. We only use the test set for evaluating our selected model to ensure having the most reliable model evaluation result before deploying the model.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY81vgPTi5gt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7754d5e-f951-4abb-e450-4925daa69d21"
      },
      "source": [
        "# Spliting the data into training (80%) and testing (20%) sets\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X = data.drop('diagnosis', axis=1)\r\n",
        "y = data['diagnosis']\r\n",
        "X_train, X_test, y_train, y_test = train_test_split (X, y,train_size = 0.8)\r\n",
        "print ('Shapes of X_train, y_train: ', X_train.shape, y_train.shape)\r\n",
        "print ('Shapes of X_test, y_test: ', X_test.shape, y_test.shape)\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shapes of X_train, y_train:  (455, 30) (455,)\n",
            "Shapes of X_test, y_test:  (114, 30) (114,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK488A8ORL5z"
      },
      "source": [
        "### Based Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgo9TUt3dAyS"
      },
      "source": [
        "# Building a Random forest model\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "base_rf = RandomForestClassifier (random_state = 42)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGr1wDBTtOWe",
        "outputId": "e5027382-ada4-48bd-9c8d-7d819fcb0d9d"
      },
      "source": [
        "# Validate the model's performance using k-fold cross validation\r\n",
        "from sklearn.model_selection import cross_validate\r\n",
        "cv = cross_validate (base_rf, X_train, y_train, cv = 10)\r\n",
        "print(\"Base model's accuracy score of 10-fold cross validation:\\n\", cv['test_score'])\r\n",
        "print(\"Base model's cross validation accuracy mean score: \\n\", cv['test_score'].mean())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base model's accuracy score of 10-fold cross validation:\n",
            " [0.95652174 0.93478261 0.95652174 0.86956522 0.97826087 0.95555556\n",
            " 1.         0.97777778 1.         0.97777778]\n",
            "Base model's cross validation accuracy mean score: \n",
            " 0.9606763285024155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYqtnuqZSg4J"
      },
      "source": [
        "The k-fold validation allows us to get a generalized estimate performance score of the model. The base model performed quite well, with an accuracy score of 0.96 on the training set. However, we still hope to improve the performance by tuning the hyperparameters. \r\n",
        "\r\n",
        "Let's see parameters used by the base model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wHy2MSwFF7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac215af-a06c-48c9-9197-84108cba68fd"
      },
      "source": [
        "# Checking current parameters of the base model\r\n",
        "print(\"Base model's parameters:\\n\", base_rf.get_params())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base model's parameters:\n",
            " {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPtJVhkde_wG"
      },
      "source": [
        "### Tunning parameters\r\n",
        "We can use RandomizedSearchCV or GridSearchCV with Cross Validation for searching the best parameters. However, RandomizedSearchCV is more suitable when we have not determined any certain parameters, except for boostrapping. \r\n",
        "\r\n",
        "Below are parameters that are most important for us to tune:\r\n",
        "\r\n",
        "\r\n",
        "*   'bootstrap': randomly sampling with replacement. We expect to use this method to generate resample of the training data for building trees\r\n",
        "*   'max_depth': the maximum depth of the tree. It is true that increasing max_depth can improve accuracy of the model on the training set, but the model perform poorly in the unseen data. To avoid the overfitting problem of the Decision Tree that is a fully-grown tree, we should limit this parameter.\r\n",
        "*   'max_features': for setting the number of features to consider when looking for the best split\r\n",
        "*   'min_sample_leaf': The minimum number of samples required to be at a leaf node. We will not want one or two sample at each leaf node.\r\n",
        "*   'min_samples_split':The minimum number of samples required to split an internal node\r\n",
        "*   'n_estimators': The number of trees in the forest. Note that it is not true that more trees the model has, the better the model performs.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gWCzalNGLSy"
      },
      "source": [
        "#### Searching the best parameters for tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkxDrfIde3hB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fd33ee2-3afd-49c7-e3f0-4581cc46f36d"
      },
      "source": [
        "# Tunning parameters\r\n",
        "\r\n",
        "# Using randomized search on hyperparameters\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "\r\n",
        "# Setting a grid of parameters to sample \r\n",
        "# Setting the number of trees in random forest classifiers\r\n",
        "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10, endpoint = True)]\r\n",
        "# Setting the number of features for each tree\r\n",
        "max_features = ['auto', 'sqrt', 'log2']\r\n",
        "# Setting the number of depth levels for each tree\r\n",
        "max_depth = [int(x) for x in np.linspace(start = 3, stop = 36, num=33, endpoint = True)]\r\n",
        "# Setting the minimum number of samples required to split an internal node\r\n",
        "min_samples_split = [5, 10, 15]\r\n",
        "# Setting the minimum number of samples required to be at a leaf node \r\n",
        "min_samples_leaf = [3, 4, 5]\r\n",
        "# Select bootstrap method for building trees \r\n",
        "bootstrap = ['True']\r\n",
        "\r\n",
        "# Create the random grid\r\n",
        "random_grid = {'n_estimators':n_estimators, 'max_features': max_features, 'max_depth': max_depth, 'min_samples_split': min_samples_split,'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\r\n",
        "print(random_grid)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36], 'min_samples_split': [5, 10, 15], 'min_samples_leaf': [3, 4, 5], 'bootstrap': ['True']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o57zWmwiCdP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e336816-247e-4b3c-fbf0-282e45129e84"
      },
      "source": [
        "# Create a base randomforest model for tuning\r\n",
        "tune_base_rf = RandomForestClassifier(random_state=42)\r\n",
        "\r\n",
        "# Create a randomized search cross validation model for searching for the best hyperparameters for the base rf model over 100 parameters combination\r\n",
        "random_search_rf = RandomizedSearchCV (estimator=tune_base_rf, param_distributions  = random_grid, random_state=42, cv = 10, n_iter=100)\r\n",
        "\r\n",
        "# fit the randomized search CV model into the training set\r\n",
        "random_search_rf.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Print the best parameters\r\n",
        "random_search_rf.best_params_"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': 'True',\n",
              " 'max_depth': 11,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_leaf': 3,\n",
              " 'min_samples_split': 5,\n",
              " 'n_estimators': 300}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHjO3BL-S1PZ"
      },
      "source": [
        "Using the cross validation randomized search method, we found the best parameters for our tuned random forest model. Hopefully, we could achieve a bit higher performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vjgI50oGC9i"
      },
      "source": [
        "### The tuned random forest model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3HHL26EGfXT",
        "outputId": "7ab00931-b54d-40f7-c9a7-236b46f2f478"
      },
      "source": [
        "# Creating a tuned random forest model with the best parameters choosen by  the cv randomized search algorithm\r\n",
        "tuned_rf = RandomForestClassifier (n_estimators = 600, min_samples_split = 10, min_samples_leaf = 5, max_features = 'log2',  max_depth = 19, bootstrap = True, random_state=42)\r\n",
        "\r\n",
        "# Validating the model using k-fold cross validation (k=10)\r\n",
        "tuned_cv = cross_validate (tuned_rf, X_train, y_train, cv = 10)\r\n",
        "print(\"The tuned model's accuracy scores in 5-fold cross validation:\\n\", tuned_cv['test_score'])\r\n",
        "print(\"The final base tuned model's cv accuracy score: \\n\", tuned_cv['test_score'].mean())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tuned model's accuracy scores in 5-fold cross validation:\n",
            " [0.93478261 0.95652174 0.95652174 0.86956522 0.97826087 0.93333333\n",
            " 0.95555556 0.97777778 0.97777778 0.97777778]\n",
            "The final base tuned model's cv accuracy score: \n",
            " 0.9517874396135266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUdiWgz-Za8C"
      },
      "source": [
        "The tuned model performed a bit better than the base model did. Therefore, we will select it as our final model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igaqaxhxr4kj"
      },
      "source": [
        "### Evaluating the selected model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri5aECaE20O6"
      },
      "source": [
        "#### With accuracy *score*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S9xtqlJWFBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38195b80-ee17-4651-f60d-8479ee28ce46"
      },
      "source": [
        "# Fit the selected model to the training set\r\n",
        "tuned_rf.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Applying the selected model to make prediction on the test set\r\n",
        "pred = tuned_rf.predict(X_test)\r\n",
        "\r\n",
        "# Observing the estimate probability of classess in the test set\r\n",
        "pred_prob = tuned_rf.predict_proba (X_test)\r\n",
        "print ('class_0','\\t', 'class_1')\r\n",
        "print(pred_prob[:10])\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class_0 \t class_1\n",
            "[[0.97826555 0.02173445]\n",
            " [0.01184623 0.98815377]\n",
            " [0.96703351 0.03296649]\n",
            " [0.98555177 0.01444823]\n",
            " [0.80077567 0.19922433]\n",
            " [0.92948357 0.07051643]\n",
            " [0.96639978 0.03360022]\n",
            " [0.99434478 0.00565522]\n",
            " [0.95318584 0.04681416]\n",
            " [0.99879696 0.00120304]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT8Pc2WW79ma"
      },
      "source": [
        "The result of the estimate probabilities show how classes were assigned. It is a nx2 array. The first column (dimension) is for class 0, and another for the class 1. In the first instance, it was assigned class 1 because it got a probability of 0.998 for this class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psAItIyVj57U",
        "outputId": "d8011f92-0036-400d-a625-fc233d30f1b8"
      },
      "source": [
        "# Evaluate the selected model on the test set with acurracy scores\r\n",
        "print('Accuracy of the selected model in the test set: {:.4f}'.format(tuned_rf.score(X_test, y_test)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the selected model in the test set: 0.9649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLjy1lDn3Fcb"
      },
      "source": [
        "#### With confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EescqEUnBCqO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "cee781bd-3231-4438-bb91-b84ce70f45ce"
      },
      "source": [
        "# Evaluate the selected model on the test set  with confusion matrix\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "conf_matrix = confusion_matrix(y_test, pred )\r\n",
        "# visualizing confusion matrix\r\n",
        "sns.heatmap(conf_matrix, annot = True)\r\n",
        "plt.xlabel ('predicted values')\r\n",
        "plt.ylabel ('actual values')\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'actual values')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpQfnpw917AJ"
      },
      "source": [
        "#### With the ROC curve\r\n",
        "We were aware of the imbalanced proportions of the two classes in previous section. Therefore, accuracy score may not make sense. Note that the Malignant (class 1) acounts a lower portion in the dataset. Even if True Positive is high, and False Negative is low, the cost of False Negative is quite high. Receiver Operating Characteristic (ROC) Curve is a good method that enables us to balance True Positive Rate (TPR) and False Positive Rate (FPR). \r\n",
        "\r\n",
        "*   TPR = TP/P = TP/(TP+FN)\r\n",
        "*   FPR = FP/N = FP/ (FP+TN)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvqaTtOrBpZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd533013-1561-469a-aeb4-13a50d169417"
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\r\n",
        "# Taking the probability of the class 1 (Malignant) on the test set\r\n",
        "pred_prob_c1 = pred_prob[:,1]\r\n",
        "\r\n",
        "# Getting True Positive Rate (tpr) and False Positive Rate (fpr)\r\n",
        "fpr, tpr, threshold = roc_curve (y_test,pred_prob_c1, pos_label = 1)\r\n",
        "\r\n",
        "# Computing Area Under the ROC Curve (roc_auc)\r\n",
        "roc_auc_score = roc_auc_score (y_test,pred_prob_c1)\r\n",
        "roc_auc_score"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9983465608465608"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "uQeS22WmQp6U",
        "outputId": "aadcc876-3912-46a1-903c-fbaf7f2c2ab3"
      },
      "source": [
        "# Visualizing the ROC Curve\r\n",
        "plt.plot( fpr, tpr)\r\n",
        "plt.plot([0,1], '--')\r\n",
        "plt.xlabel('False Positive Rate')\r\n",
        "plt.ylabel('True Positive Rate')\r\n",
        "plt.title (\"ROC Curve (AUC = {:.3f})\".format(roc_auc_score))\r\n",
        "plt.grid()\r\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}